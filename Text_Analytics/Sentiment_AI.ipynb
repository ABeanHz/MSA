{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f719316-2bca-4b07-a075-aadb5fd0d9e7",
   "metadata": {
    "id": "5f719316-2bca-4b07-a075-aadb5fd0d9e7"
   },
   "source": [
    "# Initial Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ec50f14-8bdc-4ffc-bd21-2a281819e2df",
   "metadata": {
    "id": "4ec50f14-8bdc-4ffc-bd21-2a281819e2df",
    "outputId": "8712534d-1e08-46eb-e0a3-4f77a537cf8f"
   },
   "outputs": [],
   "source": [
    "# Installation of packages if needed\n",
    "#!pip install nltk\n",
    "#!pip install pandas\n",
    "#!pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab46e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\agbea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\agbea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\agbea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download( 'vader_lexicon' )\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "# base_url = \"https://www.roic.ai/quote/{company}/transcripts/{year}-year/{quarter}-quarter\"  -- for reference this is where the data is scraped from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15260809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "    #os.chdir(\"c:\")\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d138b",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b158af",
   "metadata": {},
   "source": [
    "The team scraped web earnings calls from roic.ac. We pulled data from a variety of industries to analyze the change in sentiment toward AI overtime. The timeframe ranged from Quarter (Q) 4 of 2021 to Q4 of 2024; however, there was not data fully available for Q4 of 2024 at the time of extraction so the final analysis finished with data from Q3 of 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e77ac00b-f1a1-4984-9e39-31315391b5fb",
   "metadata": {
    "id": "e77ac00b-f1a1-4984-9e39-31315391b5fb",
    "outputId": "8c38ccd6-1708-4b9b-ae70-8d82c4fa4546"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>earnings_date</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>PFE</th>\n",
       "      <th>GSK</th>\n",
       "      <th>MRNA</th>\n",
       "      <th>F</th>\n",
       "      <th>GM</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>PYPL</th>\n",
       "      <th>JPM</th>\n",
       "      <th>SQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024_Q4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Operator\\n\\nGood afternoon.\\n\\n \\n\\nMy name is...</td>\n",
       "      <td>Operator\\n\\nGreetings and welcome to the Micro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024_Q3</td>\n",
       "      <td>\\n\\n\\nSuhasini Chandramouli\\n\\nGood afternoon ...</td>\n",
       "      <td>Operator\\n\\nGood afternoon.\\n\\n \\n\\nMy name is...</td>\n",
       "      <td>Operator\\n\\nGreetings and welcome to the Micro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024_Q2</td>\n",
       "      <td>\\n\\n\\nSuhasini Chandramouli\\n\\nGood Afternoon,...</td>\n",
       "      <td>Operator\\n\\nGood afternoon.\\n\\n \\n\\nMy name is...</td>\n",
       "      <td>Operator\\n\\nGreetings, and welcome to the Micr...</td>\n",
       "      <td>Operator\\n\\nGood day, everyone, and welcome to...</td>\n",
       "      <td>\\n\\n\\nNick Stone\\n\\nHello everyone.\\n\\n \\n\\nWe...</td>\n",
       "      <td>Operator\\n\\nGood day and thank you for standin...</td>\n",
       "      <td>Operator\\n\\nGood day, everyone.\\n\\n \\n\\nMy nam...</td>\n",
       "      <td>Operator\\n\\nGood morning, and welcome to the G...</td>\n",
       "      <td>\\n\\n\\nTravis Axelrod\\n\\nGood afternoon, everyo...</td>\n",
       "      <td>Operator\\n\\nGood morning and welcome to PayPal...</td>\n",
       "      <td>Operator\\n\\nGood morning ladies and gentlemen....</td>\n",
       "      <td>Operator\\n\\nGood day, ladies and gentlemen, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  earnings_date                                               AAPL  \\\n",
       "0       2024_Q4                                                NaN   \n",
       "1       2024_Q3  \\n\\n\\nSuhasini Chandramouli\\n\\nGood afternoon ...   \n",
       "2       2024_Q2  \\n\\n\\nSuhasini Chandramouli\\n\\nGood Afternoon,...   \n",
       "\n",
       "                                                NVDA  \\\n",
       "0  Operator\\n\\nGood afternoon.\\n\\n \\n\\nMy name is...   \n",
       "1  Operator\\n\\nGood afternoon.\\n\\n \\n\\nMy name is...   \n",
       "2  Operator\\n\\nGood afternoon.\\n\\n \\n\\nMy name is...   \n",
       "\n",
       "                                                MSFT  \\\n",
       "0  Operator\\n\\nGreetings and welcome to the Micro...   \n",
       "1  Operator\\n\\nGreetings and welcome to the Micro...   \n",
       "2  Operator\\n\\nGreetings, and welcome to the Micr...   \n",
       "\n",
       "                                                 PFE  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Operator\\n\\nGood day, everyone, and welcome to...   \n",
       "\n",
       "                                                 GSK  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  \\n\\n\\nNick Stone\\n\\nHello everyone.\\n\\n \\n\\nWe...   \n",
       "\n",
       "                                                MRNA  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Operator\\n\\nGood day and thank you for standin...   \n",
       "\n",
       "                                                   F  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Operator\\n\\nGood day, everyone.\\n\\n \\n\\nMy nam...   \n",
       "\n",
       "                                                  GM  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Operator\\n\\nGood morning, and welcome to the G...   \n",
       "\n",
       "                                                TSLA  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  \\n\\n\\nTravis Axelrod\\n\\nGood afternoon, everyo...   \n",
       "\n",
       "                                                PYPL  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Operator\\n\\nGood morning and welcome to PayPal...   \n",
       "\n",
       "                                                 JPM  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  Operator\\n\\nGood morning ladies and gentlemen....   \n",
       "\n",
       "                                                  SQ  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  Operator\\n\\nGood day, ladies and gentlemen, an...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"Text_Analytics/earnings_df.csv\")  ### put your directory here\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfa868-8efb-467f-981e-4559d7d6846c",
   "metadata": {
    "id": "70dfa868-8efb-467f-981e-4559d7d6846c"
   },
   "source": [
    "# Define Function\n",
    "\n",
    "This function will read in specific company columns from the dataframe. The function makes each sentence it's own observation and make everything lowercase for consistency. The data gets filtered to just sentences that contain the word \"ai\" or \"artificial intelligence\"\n",
    "\n",
    "Also, within the function, the SentimentIntensityAnalyzer() from NLKT is applied to the final sentences containing ai-related terms. A sentiment score is given to each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884f50ac-787e-4b94-8e43-72796cbc99fa",
   "metadata": {
    "id": "884f50ac-787e-4b94-8e43-72796cbc99fa"
   },
   "outputs": [],
   "source": [
    "# DEFFINE A FUNCTION TO USE ON EACH COLUMN FOR SENTIMENT ANALYSIS.\n",
    "def AI_sentiment_extraction(column):\n",
    "    #~~~~~~~~~~~~~~CLEANING~~~~~~~~~~~~~~~#\n",
    "    # Cleaning the dataframe\n",
    "    company_df = df[['earnings_date', column]]\n",
    "    company_df.head()\n",
    "    company_df = company_df.dropna()  ### dropping NaNs because this can cause problems during the sentiment analysis\n",
    "\n",
    "    # converting everything to lowercase to avoid case matching\n",
    "    company_df[column]= company_df[column].str.lower()\n",
    "    #print(\"Checking lowercase:\", company_df.head(n=5))\n",
    "\n",
    "    #~~~~~~~~~~~SPLIT INTO SENTENCES~~~~~~~~~~~~~~~#\n",
    "    # Function to split a paragraph into sentences\n",
    "    def split_into_sentences(paragraph):\n",
    "        return nltk.sent_tokenize(paragraph)\n",
    "\n",
    "    # Apply the function to split sentences and explode them into separate rows\n",
    "    company_df['sentences'] = company_df[column].apply(split_into_sentences)\n",
    "    company_split = company_df.explode('sentences').reset_index(drop=True)\n",
    "\n",
    "    # Drop the original column with the full paragraph, keep other columns and split sentences\n",
    "    company_split  = company_split[['earnings_date', 'sentences']]  # Keeping 'quarter' and the split sentences\n",
    "    #print(\"Dataframe shape:\", company_split.shape)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~COUNT BY KEY AI WORDS~~~~~~~~~~~~~~~~~~~#\n",
    "    # Create counts of key words in the observations\n",
    "    company_split['AI_ct'] = company_split['sentences'].str.count(r'\\bai\\b')\n",
    "    company_split['Artificial_ct'] = company_split['sentences'].str.count(r'\\bartificial intelligence\\b')\n",
    "    company_split['Total_AI_Terms_ct'] = company_split['AI_ct'] + company_split['Artificial_ct']\n",
    "\n",
    "    company_overtime = company_split ### going to use later on to have counts by quarter\n",
    "    company_split.head(n=25)\n",
    "\n",
    "    # Filter to just AI terms\n",
    "    company_split = company_split[company_split['Total_AI_Terms_ct'] > 0].reset_index() ### resetting index for cleaner df\n",
    "    #print(\"Checking that filter is working\",company_split.head(n=5))\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~ANALYZE SENTIMENTS~~~~~~~~~~~~~~~~~#\n",
    "    #  Convert to sentences, create VADER sentiment analyzer\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "    #  Define a function to analyze sentiment\n",
    "    def find_sentiment(text):\n",
    "        return sentiment.polarity_scores(text)['compound']\n",
    "\n",
    "\n",
    "    company_split['sentiment'] = company_split['sentences'].apply(find_sentiment)\n",
    "    #print(\"Checking that sentiment is there\",company_split.head(n=5))\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~COUNTS OVERTIME~~~~~~~~~~~~~~~~~~~~~#\n",
    "    company_overtime\n",
    "    company_overtime = company_overtime.groupby(\"earnings_date\")['Total_AI_Terms_ct'].sum().reset_index()\n",
    "    #print(company_overtime)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~RETURN DF for ANALYSIS~~~~~~~~~~~~~~~~#\n",
    "    return company_split, company_overtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5920d-0de9-4d37-a419-1905d99ce188",
   "metadata": {
    "id": "08e5920d-0de9-4d37-a419-1905d99ce188"
   },
   "source": [
    "# Apply sentiment function to each company\n",
    "This runs the above function on each column/company to clean and apply a sentiment score to each company.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c8917d-091c-4cfb-831e-1d460a935a6b",
   "metadata": {
    "id": "60c8917d-091c-4cfb-831e-1d460a935a6b"
   },
   "outputs": [],
   "source": [
    "# Assuming you have a list of companies\n",
    "companies = ['AAPL', 'NVDA', 'MSFT', 'PFE', 'GSK', 'MRNA', 'F',\t'GM', 'TSLA', 'PYPL', 'JPM', 'SQ']  # Add more companies as needed\n",
    "\n",
    "# Setting industries in a dictionary\n",
    "industry_map = {\n",
    "    'AAPL': 'tech',\n",
    "    'NVDA': 'tech',\n",
    "    'MSFT': 'tech',\n",
    "    'PFE': 'pharma',\n",
    "    'GSK': 'pharma',\n",
    "    'MRNA':'pharma',\n",
    "    'F': 'automotive',\n",
    "    'GM': 'automotive',\n",
    "    'TSLA': 'automotive',\n",
    "    'PYPL': 'financial services',\n",
    "    'JPM': 'financial services',\n",
    "    'SQ': 'financial services'\n",
    "}\n",
    "# Initialize lists to store results\n",
    "sentiment_dfs = []\n",
    "overtime_dfs = []\n",
    "\n",
    "for company in companies:\n",
    "    sentiment, overtime = AI_sentiment_extraction(company)\n",
    "\n",
    "    # Add industry and company columns\n",
    "    sentiment['industry'] = industry_map.get(company, 'unknown')  # Default to 'unknown' if not found\n",
    "    sentiment['company'] = company\n",
    "\n",
    "        # Add industry and company columns\n",
    "    overtime['industry'] = industry_map.get(company, 'unknown')  # Default to 'unknown' if not found\n",
    "    overtime['company'] = company\n",
    "\n",
    "    # Append to lists\n",
    "    sentiment_dfs.append(sentiment)\n",
    "    overtime_dfs.append(overtime)\n",
    "\n",
    "# Optionally, concatenate all sentiment and overtime DataFrames\n",
    "all_sentiment = pd.concat(sentiment_dfs, ignore_index=True)\n",
    "all_overtime = pd.concat(overtime_dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb833a4-c8f5-488a-a304-bd8afe5c1fd8",
   "metadata": {
    "id": "fbb833a4-c8f5-488a-a304-bd8afe5c1fd8"
   },
   "outputs": [],
   "source": [
    "all_sentiment.to_excel(\"Text_Analytics/all_company_sentiments.xlsx\", sheet_name='All Company Sentiment', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f197ede-62f3-4ea1-906e-f084206c88c5",
   "metadata": {
    "id": "9f197ede-62f3-4ea1-906e-f084206c88c5"
   },
   "outputs": [],
   "source": [
    "all_overtime.to_excel(\"Text_Analytics/all_company_overtime.xlsx\", sheet_name ='Overtime', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Update_P3_13_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
